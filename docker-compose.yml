services:
  vllm_qwen3_32B2:
    platform: linux/amd64
    image: vllm/vllm-openai:v0.11.0
    container_name: vllm_qwen3_32B2
    ports:
      - ${LLM_API_PORT}:8000
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    ipc: host
    volumes:
      - ${HUGGING_FACE_CACHE_DIR}:/root/.cache/huggingface
    command: >
      --port 8000
      --model Qwen/Qwen3-32B-AWQ
      --max-model-len 10000
      --max-num-seqs 1
      --kv-cache-dtype fp8
      --gpu-memory-utilization 0.95
      --enable-auto-tool-choice
      --tool-call-parser hermes
      --tensor-parallel-size 1
      --reasoning-parser qwen3
      --uvicorn-log-level warning
      --disable-log-requests
  
  faster_whisper:
    build:
      context: https://github.com/DCC-BS/bentoml-faster-whisper.git
      dockerfile: ./Dockerfile
      platforms:
        - linux/amd64
        - linux/arm64
    ports:
      - '50001:50001'
    environment:
      - http_proxy
      - HTTP_PROXY
      - https_proxy
      - HTTPS_PROXY
      - no_proxy
      - NO_PROXY
      - TIMEOUT
      - MAX_CONCURRENCY
      - MAX_BATCH_SIZE
      - MAX_LATENCY_MS
      - HF_AUTH_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - hugging_face_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          memory: 16g
          devices:
            - driver: nvidia
              device_ids: [ '1' ]
              capabilities: [ gpu ]
  docling-serve:
    image: quay.io/docling-project/docling-serve-cu124:latest
    container_name: docling-serve
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '1' ]
              capabilities: [gpu]
    ports:
      - "8004:5001"
    environment:
      - DOCLING_MODELS_PATH=/models
      - DOCLING_CONFIG_FILE=/app/docling_config.json
      - DOCLING_SERVE_ENABLE_UI=true
    volumes:
      - "${HOME}/.cache/docling:/models"
      - ./docling_config.json:/app/docling_config.json:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

volumes:
  hugging_face_cache: